{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUcED0VHft/gOp522JRSaf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rinogrego/Learning-LLM/blob/main/exploration/LlamaIndex-Exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring LlamaIndex"
      ],
      "metadata": {
        "id": "F09McKTKiGGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment API Keys"
      ],
      "metadata": {
        "id": "1id7ydiVKJ0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "JJ9XKHLgKLHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HuggingFace LLM - Camel-5b\n",
        "\n",
        "Ref: https://docs.llamaindex.ai/en/stable/examples/customization/llms/SimpleIndexDemo-Huggingface_camel.html"
      ],
      "metadata": {
        "id": "7jQxmttlktyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index-llms-huggingface\n",
        "!pip install llama-index\n",
        "\n",
        "from google.colab import output\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "rjERWdQslvXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core import Settings"
      ],
      "metadata": {
        "id": "8Ru_TCLDlvMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading Data"
      ],
      "metadata": {
        "id": "tBykTUVPppUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p 'data/paul_graham/'\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1NC5LiAlvGt",
        "outputId": "6ec4d0e5-f036-4c66-a65b-301cdc226080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-03 12:03:31--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: ‘data/paul_graham/paul_graham_essay.txt’\n",
            "\n",
            "data/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2024-03-03 12:03:31 (46.7 MB/s) - ‘data/paul_graham/paul_graham_essay.txt’ saved [75042/75042]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load documents\n",
        "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()"
      ],
      "metadata": {
        "id": "m53TKSEslvA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up Prompt Template"
      ],
      "metadata": {
        "id": "JxTzz7oRpmq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup prompts - specific to StableLM\n",
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "# This will wrap the default prompts that are internal to llama-index\n",
        "# taken from https://huggingface.co/Writer/camel-5b-hf\n",
        "query_wrapper_prompt = PromptTemplate(\n",
        "    \"Below is an instruction that describes a task. \"\n",
        "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "    \"### Instruction:\\n{query_str}\\n\\n### Response:\"\n",
        ")"
      ],
      "metadata": {
        "id": "1pnnQ7cmlu6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up LLM"
      ],
      "metadata": {
        "id": "Iidy3oF6pkef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=2048,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0.25, \"do_sample\": False},\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=\"Writer/camel-5b-hf\",\n",
        "    model_name=\"Writer/camel-5b-hf\",\n",
        "    device_map=\"auto\",\n",
        "    tokenizer_kwargs={\"max_length\": 2048},\n",
        "    # uncomment this if using CUDA to reduce memory usage\n",
        "    model_kwargs={\n",
        "        \"torch_dtype\": torch.float16,\n",
        "        \"offload_folder\": \"offload\"\n",
        "    }\n",
        ")\n",
        "\n",
        "Settings.chunk_size = 512\n",
        "Settings.llm = llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308,
          "referenced_widgets": [
            "ce77f1a8b9e24c36ab4efaaf9e3a7518",
            "308e9589c81843ec89d76039cdd8a9c6",
            "0dd3e88dec9241c3ad7716a8a0246e47",
            "604228c80fa24c6ca4f47a602898a88e",
            "84b6a7fb5a25408e80e165c2d9820b9e",
            "3e2601ca62b242ed8548df5ca48acfcb",
            "a24e372c3f0c4dc59112d6d9c5a8c9ad",
            "968f91a8e1084120a9c5904f03101993",
            "2b67fd72a76c46c0aeedee127c36b9ef",
            "aea4ebd4c90a447b9a44ec44635d66e4",
            "a9ffaefb2cd84dac9f6cc119041fc3b3",
            "766a15bf414d49f1a31cbac22e3f339d",
            "f8936ea415dd415abf15d27228851e81",
            "ef9b1d4033e84781b9b98e1e4a0e95b3",
            "c6329710a300465f8c41067df0e3f1ba",
            "450a9405925e44cebaa57bed80e36fed",
            "ff083d97f73147b49a6f3d9bd36a9d81",
            "88250281e8b04429b06daf3961357e8e",
            "2a4ccd4cca75450c9063233de136e449",
            "407e98f0a73d43d3b2f9096454f63f27",
            "bf8b8bcda2154f31a6fdc73a0110fa5d",
            "99be1617761842c8b162a605406e6972",
            "1470f4ad274f4513afd1140ba8ce9285",
            "ddf851d5e7b44f7482e2127a8bf9a0e0",
            "d56d62aa58ba4b8a8af6adb26d222536",
            "9dd6ea9e9e2148ffa4011b9e44b82548",
            "cecf0472eae94924894d69c90476dae2",
            "fd36b09ba3fc4cab9676db907640f01c",
            "e2107080c0f94ba3a48cf60bdca872b2",
            "7738464e23364862bccebf73a762faf3",
            "d58fbe8c69dc45f3906811d55526d416",
            "f9b6ee5c96dd489d9af890798bcd8f7e",
            "d9c80fe528b6431da00493ceaa013d11",
            "25522080181d4189beaa686382693cf0",
            "4eaaa3d647be484bbd9a8c40345dd3e9",
            "f6e01d5be9f54e7ab6ab8e6d00ef62db",
            "07a2f26fe49d4db18d5680692ead3a9d",
            "dc670fd13fd84943b1a95f4fbd8302ff",
            "c8622d2794fd43b69c57119c4aa19dc1",
            "986339fcb8c44ec680e6dccd6b1b9a04",
            "0835a4860656401881c4dfc329b8f363",
            "8214c29306e34cf9b65314622fe2dbcc",
            "702d741ed50f4a588d337f42ee74ecf8",
            "ba3a2bd44ab74ef49b5462bc03cfdaf3",
            "9d97a185385f48698ca2fdd59a8c4a1e",
            "02b40f1c296b44969420d1047ad2537c",
            "bd23eda0553845c2ae078565796070f6",
            "0af2365e7e0e471a88d1bcf255928f63",
            "af1c9cbb81524beabe84b4d4d23fbaa7",
            "13604b13907441c1b7a72f05ce7e2f8c",
            "623192431438487da62a8cd54919f138",
            "644c8b75c00545a6889ab762dee87d04",
            "626bc4b14d4946bc8fa58f97b3f6d2ee",
            "73b7507bcda14208b5fa728520c61315",
            "9aea6c997dc043348f3af1af5d69ca02",
            "8b8e06c0ab55401796e30b4626a8f9e1",
            "5f55ff6dc3af404f878d93994b920af3",
            "e3fcf72c3df6459c8ff0eea051a32d10",
            "574737aa43434c36819d136be9414f62",
            "68d138fa2de84205883c92a8ba61cf32",
            "b2904cc7fc2043b59a99331eac0194c6",
            "fc9594ff2684449eb52a3ff8cf9fef73",
            "87f704d4780b4df1bc1c6c2acff01ca7",
            "f6a746dc150d4dd8bb50eac25e9ac21d",
            "ac9d7b66754b40ce8b2308d035338cd8",
            "b1d32e01b44845cfb737b6d5d4184706",
            "a9af7dbf26724a91a89de838e37456f3",
            "5d827fbf2773483c9c75eb0a0cc28b73",
            "ebf29e83d1624cb0b43d74fd09845991",
            "15d8c2f154fa47ed825b395413841c24",
            "0fb8acb2fa9a446eb467245eb9fb9a38",
            "15afe9316734481ab3f6bd5979bd8fa9",
            "96fa279528f24cddacf56edc92ad369c",
            "9b37bcd6b0844330bad027cf684f82ed",
            "e542e0eaf60c426d81a7224563da4cd5",
            "3ae340fa3f7b452ca05bc1f6d61ddacd",
            "de6b585137cf4d82bd1b984c837928de",
            "3ec706fddcdd4b9d88c232c107c2edde",
            "6101128f09d2487abc6f86b67d62f9c7",
            "606e6fd059884c6f9dc3dd6def65030a",
            "1402d94926e842f8ad0118d235aa9f03",
            "32dd9b2ac8b449578c46b08ef1f4e054",
            "f2b3b35a764f40fda3722f9a93dbf0f5",
            "b58dd788f7f74893877db6f972629b1a",
            "b37ff770f225488283792533852ca9c4",
            "e2dbc2bb61ae4b0ba5988b6d0fb9a7e7",
            "ee8670533d434e52b834099baaf65a1a",
            "3ec030dd84e441ffb5b4647ed0324d67"
          ]
        },
        "id": "Hac_7TK0mLqz",
        "outputId": "96b5fde3-8001-47c6-8649-7b387bb5b6bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce77f1a8b9e24c36ab4efaaf9e3a7518"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "766a15bf414d49f1a31cbac22e3f339d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/748 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1470f4ad274f4513afd1140ba8ce9285"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25522080181d4189beaa686382693cf0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d97a185385f48698ca2fdd59a8c4a1e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b8e06c0ab55401796e30b4626a8f9e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9af7dbf26724a91a89de838e37456f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ec706fddcdd4b9d88c232c107c2edde"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "id": "sLmEzhB-mLm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query Index"
      ],
      "metadata": {
        "id": "MbjF0LvQmXvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set Logging to DEBUG for more detailed outputs\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What did the author do growing up?\")"
      ],
      "metadata": {
        "id": "MFDfbID8mLiz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84083111-8c4c-43df-bb71-51a615b08fdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (983 > 512). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.25` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "IHTjhH_xmLeZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e0fd602-c25c-44e5-da45-50752b425bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The author grew up in Italy, where he learned to string together a lot of abstract concepts with a few simple verbs, which led to his interest in everyday words differing from their Italian cognates.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query Index - Streaming"
      ],
      "metadata": {
        "id": "8IiVheBumhlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine(streaming=True)"
      ],
      "metadata": {
        "id": "ebxF1-cRmLZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set Logging to DEBUG for more detailed outputs\n",
        "response_stream = query_engine.query(\"What did the author do growing up?\")"
      ],
      "metadata": {
        "id": "ulTkVnZgmcFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6737c5f7-1e49-4af5-ef95-7a31f4b9d97b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# can be slower to start streaming since llama-index often involves many LLM calls\n",
        "response_stream.print_response_stream()"
      ],
      "metadata": {
        "id": "rMx0uCY-mcAd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8985fdc3-164f-43ba-bc8d-ccb9f9de8330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The author grew up in Italy, where he learned to string together a lot of abstract concepts with a few simple verbs, which led to his interest in everyday words differing from their Italian cognates.<|endoftext|>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# can also get a normal response object\n",
        "response = response_stream.get_response()\n",
        "print(response)"
      ],
      "metadata": {
        "id": "e-cua0Sqmb7R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "651c02c5-7163-4b1c-8901-6b88c2c96365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The author grew up in Italy, where he learned to string together a lot of abstract concepts with a few simple verbs, which led to his interest in everyday words differing from their Italian cognates.<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# can also iterate over the generator yourself\n",
        "generated_text = \"\"\n",
        "for text in response.response:\n",
        "    generated_text += text\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "0OwDZrzbmfL0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bf17300-26ed-42fc-a986-3a481bf074c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The author grew up in Italy, where he learned to string together a lot of abstract concepts with a few simple verbs, which led to his interest in everyday words differing from their Italian cognates.<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HuggingFace LLM - StableLM\n",
        "\n",
        "Ref:\n",
        "https://docs.llamaindex.ai/en/stable/examples/customization/llms/SimpleIndexDemo-Huggingface_stablelm.html"
      ],
      "metadata": {
        "id": "3eGi1B8pjMpK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation"
      ],
      "metadata": {
        "id": "KRSlUL-qjV6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index-llms-huggingface\n",
        "!pip install llama-index\n",
        "\n",
        "from google.colab import output\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "x1yP81muiEn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core import Settings"
      ],
      "metadata": {
        "id": "MsfNnn6Hi_a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading Data"
      ],
      "metadata": {
        "id": "R6RRtJdgjX4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p 'data/paul_graham/'\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQrvaVrmjAay",
        "outputId": "0931dd82-20bb-4421-b6da-5238d44fd01d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-03 12:56:55--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: ‘data/paul_graham/paul_graham_essay.txt’\n",
            "\n",
            "data/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2024-03-03 12:56:55 (35.0 MB/s) - ‘data/paul_graham/paul_graham_essay.txt’ saved [75042/75042]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load documents\n",
        "documents = SimpleDirectoryReader(\"./data/paul_graham\").load_data()"
      ],
      "metadata": {
        "id": "oQP9CXbOjCTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up Prompt Template"
      ],
      "metadata": {
        "id": "ZnDgyFvIjb_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup prompts - specific to StableLM\n",
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
        "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
        "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
        "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
        "- StableLM will refuse to participate in anything that could harm a human.\n",
        "\"\"\"\n",
        "\n",
        "# This will wrap the default prompts that are internal to llama-index\n",
        "query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")"
      ],
      "metadata": {
        "id": "FG2QhhRCjDkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up LLM"
      ],
      "metadata": {
        "id": "Q_rsZeh1jd8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n",
        "    model_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n",
        "    device_map=\"auto\",\n",
        "    stopping_ids=[50278, 50279, 50277, 1, 0],\n",
        "    tokenizer_kwargs={\"max_length\": 4096},\n",
        "    # uncomment this if using CUDA to reduce memory usage\n",
        "    # Ref offload_folder: https://github.com/nomic-ai/gpt4all/issues/239\n",
        "    # Ref offload_folder: https://huggingface.co/tiiuae/falcon-7b/discussions/82\n",
        "    model_kwargs={\"torch_dtype\": torch.float16, \"offload_folder\": \"offload\"},\n",
        ")\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.chunk_size = 1024"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510,
          "referenced_widgets": [
            "6076f1cedc7b4939850d01806d0dd289",
            "2f073cf53ff045b48144b1b6c55e8f53",
            "8de6a6d4e7c84cb2bb4f9ad54a59a546",
            "7e01ffb62c2e491a8429bec2d48d1cf9",
            "59793ca3ed4648be88b4081eecc34b07",
            "ea53b556872a49948b49be8c5cada808",
            "e142fe91fdee40aea8a3fc174afa98f9",
            "66b6083108624dbb899f47736cb267b3",
            "18a9cb652950435db5204da7debcd2ca",
            "fa4e024090e64d5d92e1220d425cac1f",
            "0067f81ea361438580d89e49627e52e5",
            "3f62206208764ed390a014e2963eb6a6",
            "2ea626b388084c6987dc6c952bbaf82e",
            "b8e1051610224d6eb4df17319f9c0dc5",
            "79e1212a7d724193a57c31fb129d335e",
            "bbb4f077a6994980a1f45cd23ba8d6d8",
            "a1fb2945fb584d089a2cf14eb3827e42",
            "b38e8491c16e4884a954e053058db0a2",
            "1755961480a64589a8edfcce585953ec",
            "d8adc88f1a4340e3bfe9b3f15430e063",
            "880cd167792f46148f7c8af34f645dfe",
            "377287052d2a4d9b9b068b77f76f4c09"
          ]
        },
        "id": "PeoQYLorjEvv",
        "outputId": "756d24f7-83e8-48c9-d46c-ccc9760c7573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6076f1cedc7b4939850d01806d0dd289"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:983: UserWarning: Not enough free disk space to download the file. The expected file size is: 10161.14 MB. The target location /root/.cache/huggingface/hub only has 4368.45 MB free disk space.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:983: UserWarning: Not enough free disk space to download the file. The expected file size is: 10161.14 MB. The target location /root/.cache/huggingface/hub/models--StabilityAI--stablelm-tuned-alpha-3b/blobs only has 4368.45 MB free disk space.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/10.2G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f62206208764ed390a014e2963eb6a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[Errno 28] No space left on device",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-d55e742e41c7>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m llm = HuggingFaceLLM(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcontext_window\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/llms/huggingface/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, context_window, max_new_tokens, query_wrapper_prompt, tokenizer_name, model_name, model, tokenizer, device_map, stopping_ids, tokenizer_kwargs, tokenizer_outputs_to_remove, model_kwargs, generate_kwargs, is_chat_model, callback_manager, system_prompt, messages_to_prompt, completion_to_prompt, pydantic_program_mode, output_parser)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;34m\"\"\"Initialize params.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mmodel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         self._model = model or AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    562\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_sharded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3263\u001b[0m             \u001b[0;31m# rsolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3264\u001b[0;31m             resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n\u001b[0m\u001b[1;32m   3265\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3266\u001b[0m                 \u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m             \u001b[0;31m# Load from URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             cached_filename = cached_file(\n\u001b[0m\u001b[1;32m   1039\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0mshard_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    399\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1455\u001b[0m                     \u001b[0m_check_disk_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m             http_get(\n\u001b[0m\u001b[1;32m   1458\u001b[0m                 \u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m                 \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, _nb_retries)\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                     \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                     \u001b[0mtemp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m                     \u001b[0mnew_resume_size\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0;31m# Some data has been downloaded from the server so we reset the number of retries.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/tempfile.py\u001b[0m in \u001b[0;36mfunc_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    620\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0m_functools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Avoid closing the file as long as the wrapper is alive,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;31m# see issue #18879.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build VectorIndexStore"
      ],
      "metadata": {
        "id": "R4NElJzUjqKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        ")"
      ],
      "metadata": {
        "id": "ftNZcWKJjI0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query Index"
      ],
      "metadata": {
        "id": "H_Q6C4DZjtnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set Logging to DEBUG for more detailed outputs\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What did the author do growing up?\")"
      ],
      "metadata": {
        "id": "2nSFYDnRjmQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "Kyc2NqPtjuTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query Index - Streaming"
      ],
      "metadata": {
        "id": "3wFun9FBjwTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine(streaming=True)"
      ],
      "metadata": {
        "id": "3ARpgl_hjv40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set Logging to DEBUG for more detailed outputs\n",
        "response_stream = query_engine.query(\"What did the author do growing up?\")"
      ],
      "metadata": {
        "id": "hyYhu_ppj0j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# can be slower to start streaming since llama-index often involves many LLM calls\n",
        "response_stream.print_response_stream()"
      ],
      "metadata": {
        "id": "i0vwLJx7j1v9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# can also get a normal response object\n",
        "response = response_stream.get_response()\n",
        "print(response)"
      ],
      "metadata": {
        "id": "IX4PAGyRj3_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# can also iterate over the generator yourself\n",
        "generated_text = \"\"\n",
        "for text in response.response:\n",
        "    generated_text += text\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "NpkbcUxij5BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain LLM\n",
        "\n",
        "https://docs.llamaindex.ai/en/stable/examples/llm/langchain.html"
      ],
      "metadata": {
        "id": "oCQVvei1p4Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index-llms-langchain\n",
        "\n",
        "from google.colab import output\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "3m-Wdz_JkgzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI"
      ],
      "metadata": {
        "id": "GM3yjxfsUsmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from llama_index.llms.langchain import LangChainLLM"
      ],
      "metadata": {
        "id": "4pwIvP1ep7qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LangChainLLM(llm=OpenAI())"
      ],
      "metadata": {
        "id": "MZzLQ6t2qCJa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb0a4d1-9d47-448d-d867-dbca711d6399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# above script displayed following error\n",
        "# /usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
        "#   warn_deprecated(\n",
        "!pip install -U langchain-openai\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "from google.colab import output\n",
        "output.clear()\n",
        "\n",
        "llm = LangChainLLM(llm=OpenAI())"
      ],
      "metadata": {
        "id": "EQoVEdfFT_om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_gen = llm.stream_complete(\"Hi. What do you know about Universitas Indonesia?\")"
      ],
      "metadata": {
        "id": "lwGle7zUqC9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for delta in response_gen:\n",
        "    print(delta.delta, end=\"\")"
      ],
      "metadata": {
        "id": "2-_zK512qDyP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "789cc825-a992-4396-ec21-ce59b3f04e3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Universitas Indonesia (UI) is a top public university located in Depok, West Java, Indonesia. It was founded in 1849 as the first university in Indonesia and is now considered the oldest and most prestigious university in the country.\n",
            "\n",
            "UI has 15 faculties, including Faculty of Law, Faculty of Economics and Business, Faculty of Medicine, and Faculty of Humanities. It also has several international programs, such as the Faculty of Social and Political Sciences International Program and the Faculty of Humanities International Program.\n",
            "\n",
            "The university has a strong reputation for academic excellence and is ranked among the top universities in Southeast Asia and the world. It is also known for its research and innovation, with many of its faculty members being recognized nationally and internationally.\n",
            "\n",
            "UI has a diverse student body, with students coming from all parts of Indonesia and from other countries as well. It offers a wide range of extracurricular activities and has a vibrant campus life.\n",
            "\n",
            "In addition, UI has partnerships with many universities and institutions worldwide, providing students with opportunities for international exchange programs and collaborations.\n",
            "\n",
            "Overall, Universitas Indonesia is a highly respected and reputable university that has produced many successful graduates in various fields. "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Embeddings with HuggingFace\n",
        "\n",
        "https://docs.llamaindex.ai/en/stable/examples/embeddings/huggingface.html"
      ],
      "metadata": {
        "id": "pm9bnNcuqaMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index-embeddings-huggingface\n",
        "%pip install llama-index-embeddings-instructor\n",
        "!pip install llama-index\n",
        "\n",
        "from google.colab import output\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "XdOOUJj_qaiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HuggingFaceEmbedding"
      ],
      "metadata": {
        "id": "rACZ99wiqr_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# loads BAAI/bge-small-en\n",
        "# embed_model = HuggingFaceEmbedding()\n",
        "\n",
        "# loads BAAI/bge-small-en-v1.5\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "47853a0d66654f81a29f75b7dc50f930",
            "7ef4369b514e475c91da4255011a76eb",
            "c27446d1d6104184b452c55d96c8d8cf",
            "cab2f17972f9475fa8d0288650d4831f",
            "37d660a165154ac986377f241c3c642e",
            "0723cbd47f3648c69aeedf237eba283c",
            "ad10ef95d2474d0980848b1dd09532e5",
            "7eba1042d8a6428e940cf308db40722f",
            "a91c593ccd494921a73591d77de04aea",
            "deaa6f5320a141cb966930f1ceb87e9e",
            "887be64040864390aad8afbafb0e2e11",
            "61a7c3af92fd4ff29cfc0f9c98891e76",
            "e28a5b047e02454f9f5506b430d85f0c",
            "45d9b0cdd5a54d089336afe91d7a4287",
            "c761b80a916c4365b374a3500e003cfb",
            "ddbf009f9e0f4d65a73382afd1e406ac",
            "05c363a892954955a15f5b8e100ef580",
            "ca68bf71386a4964b216a9811092a6f7",
            "d270e0b6adbd453eb70cfccda8d9ad45",
            "dd248d63cfde4b7abd36c63489bd66bd",
            "0023535737ac498ab4aa15271793a4c7",
            "544377eda9524563ac9c2ec775f42aa1",
            "fcea3528fa3e46c08475da73ee286e8e",
            "868dd0ba54b74aed81fd9ea4116e2e5b",
            "484d2a38867e445e97c322d840de7e11",
            "7570042b22f74dceb5b46baf5eae7cd9",
            "4d91282b720d4c998662a48a60c8aaf9",
            "2f147430fe1b4a35bdfd4155f9dbbbb7",
            "9029e1ec34d641b1b76e9a46b49eff75",
            "a8994f257d404038b7155a3eaa8ac105",
            "34d5d5ae8d444233ac1edc16b0c878d8",
            "dd806aa6c4e043618f0f50ec758ae948",
            "650806282c9741fa9867e055d98eb336",
            "fc51cbbd346c4276aba86425e1b939a9",
            "3dc9dadccd8f41849dbef1e371db5a83",
            "aa4324a3bfd44fdbb3430da504e36d2b",
            "55f2f9b03239482ebefb2230d6a0ba3a",
            "69e0f34051d74512ba13e7ab70965414",
            "b70cb3e66a174e778fdd8f43a9fb58db",
            "dc5b461afa6b4d1a9f0cf6113918c0f0",
            "1d9af13d3662453a89976f62fdca8376",
            "3f07cff1759443658f5f2a6756441151",
            "27edfdc07e63436eb6f4ed986e006ca7",
            "ebd8b9f39dd44148a503a8390dade020",
            "4658062c97e64a7499c3784ed5067613",
            "45921c54d4914256a8bb8fd15dadd71a",
            "c6a83e8ff4074f20bb353b00515bcd13",
            "3a536a3c0e8d4679845d6465164bcd42",
            "8bb3c21a14f74b03b90910b755b521df",
            "3c6e47ef5dd945d4b662f6e97ebec0d1",
            "9aa2886b2429460880996d9b4dfb2980",
            "4951b94c4a6e4394b8d46e6cfd95d6de",
            "3feb52bc9c9e4ced9f018d9dfdf7637d",
            "dc1dfeea67f440c98d952440d1390f32",
            "d636279bf9b940e78654e09c468b0cea",
            "02d880726e40449a92dc308e3aa4b297",
            "efa96a1d72b44ec3a1b92b528f0cd55e",
            "9288f24fbd4443a1ac333d500307603a",
            "70ce081e77b544ff817e3c1d5aab7f46",
            "d55c84ad44ae4873a4fe83d6f1ad6174",
            "cac56ef657894d10a7f8ba631fb54b92",
            "574265e980614941ade0df54bfbdf3cb",
            "3755e858d3be4f9183c2753384893d20",
            "7fd7c2569a7b4c42b5dc40af61378886",
            "6059f14298ad403e8133c6db0cf6ee29",
            "5bcd4c6395ed48d5b716ca8c5ef5e536"
          ]
        },
        "id": "upUK0SDhqkRe",
        "outputId": "9c177844-7f0f-47c4-cacb-a83ccb80de6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47853a0d66654f81a29f75b7dc50f930"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61a7c3af92fd4ff29cfc0f9c98891e76"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fcea3528fa3e46c08475da73ee286e8e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc51cbbd346c4276aba86425e1b939a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4658062c97e64a7499c3784ed5067613"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02d880726e40449a92dc308e3aa4b297"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = embed_model.get_text_embedding(\"Hello World!\")\n",
        "print(len(embeddings))\n",
        "print(embeddings[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEd8_yAiqnIV",
        "outputId": "f33ec5d6-d486-446b-9afe-6dde43666720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "384\n",
            "[-0.003275663824751973, -0.011690725572407246, 0.04155917093157768, -0.03814810514450073, 0.02418305166065693]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### InstructorEmbedding\n",
        "\n",
        "Instructor Embeddings are a class of embeddings specifically trained to augment their embeddings according to an instruction. By default, queries are given `query_instruction=\"Represent the question for retrieving supporting documents: \"` and text is given `text_instruction=\"Represent the document for retrieval: \"`."
      ],
      "metadata": {
        "id": "ZApO6QTrqtNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install InstructorEmbedding\n",
        "!pip install -U sentence-transformers\n",
        "\n",
        "from google.colab import output\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "Tgzvpg9MqyQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aOsaDm5UVlg",
        "outputId": "0f81882b-8b44-463c-8a72-ffde40ee7bb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: sentence-transformers\n",
            "Version: 2.5.1\n",
            "Summary: Multilingual text embeddings\n",
            "Home-page: https://www.SBERT.net\n",
            "Author: Nils Reimers\n",
            "Author-email: info@nils-reimers.de\n",
            "License: Apache License 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: huggingface-hub, numpy, Pillow, scikit-learn, scipy, torch, tqdm, transformers\n",
            "Required-by: llama-index-finetuning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers==2.2.2\n",
        "%pip install llama-index-embeddings-instructor\n",
        "!pip install llama-index\n",
        "\n",
        "# to handle following issue\n",
        "# TypeError: INSTRUCTOR._load_sbert_model() got an unexpected keyword argument 'token'\n",
        "# ref: https://github.com/run-llama/llama_index/issues/11037#issuecomment-1954720330"
      ],
      "metadata": {
        "id": "mN6YwCmxW1iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.instructor import InstructorEmbedding\n",
        "\n",
        "embed_model = InstructorEmbedding(model_name=\"hkunlp/instructor-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659,
          "referenced_widgets": [
            "1c273e61eab74d319c3f450c6bff6641",
            "2d316fd09d9b4c86a497e93fb6d165cd",
            "c30e4f8056184d08842e4af647d43ed7",
            "d83d00a6492343e9ac9d41c5d1d8932e",
            "8eaa2a193acf43c59bcbe22e150d4e37",
            "380ddc0836f546be8cde6cbbbe638294",
            "1b321eb9c8544185943d3f6a4c85e41e",
            "0b4e5536d71143ef9f12bfc898894be3",
            "8c55a99043764d678d41d424931f4fb3",
            "c90c31b67fce4b2bba6b47ece6b94bc0",
            "f701f18d24e840428dd8b2abbef72ee2",
            "fd99b735793c46519cc7a3fe57e05c66",
            "e7263ff30497421f9631863b3b35061d",
            "7dacd15c1f5c4fa2a8c342f80c1113b4",
            "bb5824e83c2540a997fe001543f59edc",
            "ff7e03794bed495c81163ddae61d3ba0",
            "6f5c25cecf3d4308af8ccf6675b72149",
            "31d0451be0c84b0db347dcc6eabc6b83",
            "f28b97aced2242d283ea38fd6a7b0e0b",
            "a0bfd53f69044da7848fc057767fa791",
            "113264e9ca9940e2a2af3e8766c10e59",
            "8b8074b39a2f4a248cc1adcd8033e1f1",
            "4216f8cb47eb4cf787a1b0b458d533c1",
            "8b93ff2c8cb94d5aba5a7a711cc759e7",
            "07006d3c759d468b8fad5faed13a4bc7",
            "929349ffad724d66b755c999ededc03c",
            "020970d3c46a4bb9bea4a2a3dcc793bd",
            "53b2b08fe81446bcaf90a80eca405b23",
            "9a2e79e6666642b4af0ebef6de5509a3",
            "9c3a2a06197a49fd843d5a12156f6bf8",
            "cb0af624e8c14fdd8c95a609501ed819",
            "e9387bcb32f74ab6829cfe67d4371702",
            "7491aa0c16a54c60aa1cc66f71584d5a",
            "28e48cfee39249b9ba0cd03caf89b82f",
            "3574b0665aab4c02b762fe233e7d8d89",
            "df1efa8981ee476a836b8fc78201ca14",
            "cfbeba3ebd7149f7bf60ab28c97be723",
            "fdb8db660e074553a55a62aae9697e4f",
            "584d5222a1c0462490f2257e682ec3b6",
            "da376afd3d40437cb91ac02f50fc1b58",
            "e27762583ce0471ba9e41940fb60ec3e",
            "fb638081656f4650af7f11e8e4696723",
            "234d4e08cf47408fbcde55912f51dfe6",
            "95de281d77ba49208a3a699e83a471ae",
            "662fed01ffab4a908d07c10e5d9f6c43",
            "339bdc54b84c4e72b6c52916ee5cd68b",
            "d8461531689b43028bef76d4b7b523b7",
            "f8d6cf81b4d2447aa80eec8816a878b7",
            "f6023512d279499086cf30ea51e3fc27",
            "ff1a9fc5be7e4390bf1d307d1c181301",
            "487d2d72b2664cf8a65f6de7409662e7",
            "f922c49a636d4afdb69f5356822905a7",
            "937557010c1449bfa50317f589adf7c7",
            "0c129a1d7d1745649c0220219f8e0d8b",
            "006f82bc2d9a4d06aab3c9302ca0c47d",
            "053c68f27db2468e8a61aaed7034b14e",
            "f8776cd6bc1040ef84536fb6c981abd5",
            "04ae8cec0e474e07b51323c3444c2d93",
            "a1104ae55b2647e3a0ef739a1ed9a17b",
            "33d5c6d6e46a4308879a1cbec8d9abdd",
            "58b7ed9a3c534a5da9c992f7aaadc288",
            "48308bf9c1104060b2b8ee6b7d68187b",
            "3dc7f23fc4b3474ca1b26f6c61eeb85e",
            "3f248ad7d297494f8315856f0ffd2529",
            "67882dd6e4de4093a699f8302c842ffc",
            "2af0fdc3da8944658d4f19c910936f3b",
            "e187b1821c8e4ed6a26055d4c3a9aa92",
            "c8fd7a5bee9649eba9dad4df1c86c701",
            "cc7d29e6c9ec497f9dbc3931eaa13f39",
            "08c9a928b07f4e2d83404be272aa0d0d",
            "c1ea433be6c545ccab2e9b3eb508c674",
            "c1a1f3f469ff4641a170ca7b98c4ddca",
            "2cdb0d367f8141849423c9db5b4a4b51",
            "126974f483f345ef83abef262ab58d3e",
            "0ea20a2fb2dd468e8efc0929b9bc79d8",
            "40d259ee84c445658094ccb57e98d9c3",
            "813f0c98d9694b0d806e4e4a75913151",
            "cabe492915044180a241f85958d372e3",
            "d7d172a5b86d4baaaafb542fa88acd49",
            "73305b0a943548579187065bae2a54c1",
            "2cf0af9bee474a67b23217b47597c0c9",
            "53681bcab292452d94ba39bfb974c3e1",
            "8e48af70d56b4d9cb6c4b3d782571343",
            "67690a8764ee46f3b1b059ae1d650082",
            "5166bdf4446e451282fd1dd9dca2ca93",
            "1c5faf026bb8471ba215c077a24063cd",
            "b9676deaa5a34396a0357ec07bb696d2",
            "f1f452a65aef43f3952878042c3ba88d",
            "2ce2da0956aa479c80c10a3489e4baea",
            "478b08967aaf4f7294ee8e737c3a89c3",
            "8c13db1760264c29bb7fa6311bd974ac",
            "e92f586cdcf64ed886c6a09afa9df928",
            "e28d45d7f7bb4846bd0f486f1fdc6ac0",
            "e96c5fce3cc9484991f8480297aceaa2",
            "cc41cb359de14d54bfacf6589827153c",
            "baa416a6e7db4501a19dd9ac587a719f",
            "0882bc58e1cf43a8ab07b7e9a858b22c",
            "cf60cc631eba44128c049d1df8b64176",
            "95e5622f1f1049a4864b8a13b787f9de",
            "af089b9a889d4a71abc98c27bf283eee",
            "6ec3fd52e3ab4124ad514802ecb3f8b4",
            "d933d12b6dd44061adc67c9204e0dbed",
            "02441bc210234613914825643055dbed",
            "31c3d5a3f3f64ccfac4d1dffa4fda1dc",
            "0184152a195f4e399283cd670d5e4184",
            "3ebe7f5d0b8e4205835fcd2831826a4f",
            "f93979ecafaa4b329c393b02e2301b24",
            "1fed771ca83d49bea0501643abcd1d3f",
            "88f132aa4c3544f8910a86bb85d67fff",
            "fb5692c8b5004029b105de7e06002e19",
            "7a9b1e1b13654d98b8564378aa28e92d",
            "ee4c5cdfad514eb7a044060c87964374",
            "b06c6641a6744149b8dc846cf6bd6467",
            "86de6223b6ed46838ec24e9666158f50",
            "1760c8d3e21f4e409b2dd53dba0423af",
            "b910e609ebfc46208dadb16aab71121d",
            "d5025fb87bd14519a18c2e5d5520d22e",
            "f6b8039e21b34bacb21ddbdce6a29f0d",
            "d93ff04c3f1e4f5abfeb703c07210389",
            "2f1b2bf70b8743bd8de8e11c1a3dcc7a",
            "a4a555acdb744b9685267d7d9b2722fb",
            "dc192dd7437845e5a6587a87f0998154",
            "8bab6f6d97f3444aab64980b5a934ee2",
            "9b2605f5c095471bab578cb383b73081",
            "19b64044ecc245d5af7575a3ef482094",
            "e9e78832fdd54d03a647e320236e7aba",
            "37038c8cc09844729381522f16200140",
            "f1577ec98a5f43a7afa2c493b849ae07",
            "603c3b1c004946bd99e4555767438796",
            "b6fbd526e17449f89252e85785571abd",
            "c4629ebc58bc4fc1a505ef35023c5fec",
            "488bd1f877924d278b310ae3a58180a9",
            "4611fb856ae748e9af658b1774893e41",
            "e53aac96b52e4ff18865a842240d28d3",
            "9539017bd6754204852a706b4cdb7796",
            "207c268e421d4105bf2f6991c92628fe",
            "9ec5fd9cc87f475aaee51b42504fd72a",
            "e12c212de6464c5189e6c3ba5deabe30",
            "4d86d675214545edb5b13f8d704c59de",
            "829e515e4a294ba7b9e9fe737585e7e2",
            "bc93c0ea752948709e16911524a1f3d8",
            "2e9f863af4a647c7ab15c39e8d7a73c6",
            "0c09924fadae4883b034f38312d581d3",
            "c26ea85ef7384790acf9f997e5f40103",
            "f57a4aad1c7448feabb3ddc7bc3ed462",
            "0522ec182e3e46c59a3d288dbf118dfa",
            "52cd6699282b462ead6269ad124dd0ab",
            "2f084cd6543f4402b39ce7a84d675c37",
            "e7c08cab7b754e32a2c4a52af5767880",
            "e85262c3d4814e72b5b76b3515c64129",
            "beb1bfd3acbf44baa8dd26226ac9431a",
            "4731e4cc35874d2096f7e6f545e745f0",
            "5f2493cd70004c0f80e0fd4a0fb230b8",
            "22f3f5989ae14b76b426fef37feabaa4"
          ]
        },
        "id": "GATiYOYbqoqn",
        "outputId": "eede00dc-b458-4b90-ffcd-8a23569eaa92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import trange\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ".gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c273e61eab74d319c3f450c6bff6641"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd99b735793c46519cc7a3fe57e05c66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "2_Dense/config.json:   0%|          | 0.00/115 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4216f8cb47eb4cf787a1b0b458d533c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28e48cfee39249b9ba0cd03caf89b82f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/66.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "662fed01ffab4a908d07c10e5d9f6c43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "053c68f27db2468e8a61aaed7034b14e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e187b1821c8e4ed6a26055d4c3a9aa92"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/439M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cabe492915044180a241f85958d372e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ce2da0956aa479c80c10a3489e4baea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af089b9a889d4a71abc98c27bf283eee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a9b1e1b13654d98b8564378aa28e92d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc192dd7437845e5a6587a87f0998154"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.43k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4611fb856ae748e9af658b1774893e41"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c26ea85ef7384790acf9f997e5f40103"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = embed_model.get_text_embedding(\"Hello World!\")\n",
        "print(len(embeddings))\n",
        "print(embeddings[:5])"
      ],
      "metadata": {
        "id": "6_DA81-5rDa_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03215621-6d95-4ef3-9919-ba51baab734b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "768\n",
            "[0.021553607657551765, -0.06098218262195587, 0.01796206459403038, 0.05490903556346893, 0.015269058756530285]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Base HuggingFace Embeddings"
      ],
      "metadata": {
        "id": "0iLNn-CzrO8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNGNxj0eXcPw",
        "outputId": "2e3757b7-a63c-41ef-aaab-ad6141ae5872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 20.7M  100 20.7M    0     0   427k      0  0:00:49  0:00:49 --:--:--  449k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.core import Settings\n",
        "\n",
        "documents = SimpleDirectoryReader(\n",
        "    input_files=[\"IPCC_AR6_WGII_Chapter03.pdf\"]\n",
        ").load_data()"
      ],
      "metadata": {
        "id": "S_qnBVmRrYHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ],
      "metadata": {
        "id": "Q0RQamdArDw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# loads BAAI/bge-small-en-v1.5\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "test_emeds = embed_model.get_text_embedding(\"Hello World!\")\n",
        "\n",
        "Settings.embed_model = embed_model"
      ],
      "metadata": {
        "id": "fR7MtW0wrSly",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "b44a512f7e474389b1c48bdb5bd7bfe9",
            "1004bf96642544cb9de03cfa64844663",
            "bf0686006743448ea234905d4add36d4",
            "d62a8b2a18e24ba5bd854fa249bb407a",
            "de115b4ccb754df2a4a7fb8786025a4b",
            "99dae2d8f5304d358fd31d0a5e2c6476",
            "ce811e7a6dc94a27ab5eab5d49322d87",
            "3f2f2bca78f54fd689e83b0ec0fab3ce",
            "1fc144c6c4214eb99a2b4adda5068598",
            "86fcad93475e41c2a46e7ff11868dd75",
            "864b05eeb1834c2cade9c4fc5e88d4cc",
            "598b41264cd54fd09c8483c96fc428e1",
            "053ba11295444273b81af90b7fe0cc36",
            "57823fc7fce64cea97c923012f0b7483",
            "f383892782f3424884de4188a6f3b830",
            "9a1f4d69682f40d18346d85281050921",
            "5ed79fc9e0a7446092770cde8997ab16",
            "b3cf566f18c3441e8f7c4f625ade2922",
            "31111de2fa0d4b68abe8dcfec3b71b9a",
            "e1c681f195f34a79b7325af3e35445dd",
            "5137e8c82f7f43eba99aea7b3e39253a",
            "3311a90f68a346b38d3f2aa0866d0f11",
            "f1e55a3d8db84231a5773eb3ae2215b2",
            "3db38e8b42d840acb38b63c05b5d6317",
            "48aaa95c6afa4f9ab4551872f6a0b6ec",
            "ecdcb006196343f0a8f344aec01888bc",
            "4b63e4e237e24e05b6f9f373d3bcd6e3",
            "e29b2ee9b3804f42bbd2144f5a1c141e",
            "42ea843b488e4041ad21e77192d3862a",
            "69f5c3ae6832456a9f3ddbdf81f3e8c0",
            "59e065f3b25b4ad4aeddb03fb4ff9ceb",
            "43fd4c29a4e848f5a6b2d5f59bab6224",
            "7fa26e4268ea48f9a638ca974284118a",
            "ab6ac3bd15c840a0a4771d8bdb66d9f2",
            "7f406b20315d43f18da3251ab3842ab2",
            "06a58d40bf244f8c972e3d9202774efa",
            "f35a5bdc091e414d862ab28fa63ce16e",
            "15f387b80b9f46a29ccd7f3e87ff3657",
            "f52e07abba0d4d37a35cd176850aa491",
            "7cdeb9d7907c47198736f0aeafd92d2b",
            "07329ad405dd4a218cb8d09579ebf2f3",
            "46a9962129354dcd999bebde19068a9e",
            "c3e393dad0f34983ae1d1618b3993491",
            "d0fc5479c063426da35ee83c436a9e59",
            "6fddc0023a504d8f94ee30176da41805",
            "7c71dc47446f443aa69d2635f439ec8f",
            "ac1c82e09a734588b62530e1d1cdbfa0",
            "f73b831e982545efbe3a6643a339c210",
            "93b6a28a1f6b4453a771274f9e35dca4",
            "8afbcfc1ecd840579b286a774e5048b5",
            "1c14740cdbfb4901bf80099988d6cc60",
            "87997d8010334eb19f7e9ff87e11a374",
            "8b1e4faa64ee4a80b3aa612c40318776",
            "23f91b4cf18c49569c1c88b9c60cb3f8",
            "b8bdf2ef25794bf4859b1b0816b405b2",
            "6e244b369d9e4ca0bd695da689c97f9b",
            "d8f032d38f0547f59533ef75167a24ef",
            "f8a34b0079f446ebab6b1d095941caed",
            "bd60ba1eb0284f50b74ee1e431c6ee1c",
            "11250200244a4db9aaba33b024bad5ed",
            "a0c0f4d58ed34869b918c49f4bb1c500",
            "148c7f4a5e974998ac48eeb504ecc7f6",
            "33c60535e10e4400b9412ccae6172bda",
            "a0e1677c43ab4664946b1c689a19fc00",
            "e43113d5a4d04fe2aa0a5097c58a2981",
            "70a12410aec34bcaadcf9a856cf76672"
          ]
        },
        "outputId": "09673017-4531-49c2-e8e2-3ef72ecd0473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b44a512f7e474389b1c48bdb5bd7bfe9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "598b41264cd54fd09c8483c96fc428e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1e55a3d8db84231a5773eb3ae2215b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab6ac3bd15c840a0a4771d8bdb66d9f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6fddc0023a504d8f94ee30176da41805"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e244b369d9e4ca0bd695da689c97f9b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -r 1 -n 1\n",
        "index = VectorStoreIndex.from_documents(documents, show_progress=True)"
      ],
      "metadata": {
        "id": "OfBdmIUxrUMl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "e7832200ff1a4adca56c849142009e99",
            "a8c41bfa15fa47449223106a2013cd81",
            "0531acdf08c746f1b60f36557d5ecec5",
            "bc43079765d24cb7874ebb048528060f",
            "b071cce4e20d48f491cba96e6bbec753",
            "39c8789d696047778fda21b4d62c1466",
            "d8f83d73f2344833b1812e3ef1cdc178",
            "e0d87a2e41344d47879fd63eb6e0b018",
            "77f1ac404bbd449c9624fd8601c966b6",
            "50fa957229e84661aee364e2def077c4",
            "00aee6b3a10543f9bc1d6c537d1328eb",
            "41768825ec8144f2a51a8254976b0c1e",
            "c4f682c50c464906aa889f3268ac5c20",
            "d881dfee0bbb42e59e7b2cd2be49126f",
            "b39a05400a4c49bcada6c14f56cd9c9e",
            "61f784ba70f14456b80640d2ad028c30",
            "d5dcf9e75f9e498787f966b834ac57b4",
            "28d01c0254804002a74cfaf8d2e5e6d4",
            "1733f3bb8fec4e05beea6f33f91f0ac5",
            "17b062d2efc8455eb67d769213ba451f",
            "8a7ef8b8becf498ba410a5f08a137d7f",
            "86ea522b9f95408b970d08e967e3967c"
          ]
        },
        "outputId": "609051ea-0edd-40af-f165-2033a1237eac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Parsing nodes:   0%|          | 0/172 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7832200ff1a4adca56c849142009e99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/459 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41768825ec8144f2a51a8254976b0c1e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.23 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents, show_progress=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "53eec2c9078f443d9f0e8772728c7650",
            "f4819e72144c47a5a272f5b09814b914",
            "0dfb342a4f5542c6b6a616e1273ca940",
            "898735a3545649268c683d7241bc2494",
            "9d813caef40f426d92646ef0ddac4d47",
            "106b6ee95ee84212bbbf4bf96894738b",
            "5bcc2e56ed1647caa97762ee76953d2e",
            "522d73aa46fe414390e9d71c1f919f3b",
            "26d0f85628334e26a782d3f653f287e5",
            "24bce62065fc4e9bb6b8a052d61a299d",
            "e16c4baf1c77432bb8c3f0d0a2d3fc00",
            "07e8f26629014497b1237202b6d3d60e",
            "d13f42d819744e01b79bc6ffd109a55f",
            "7a36fff83dcb434a85a184464ca2cf8d",
            "3188b7f47efd42d2b4f2a8245fb07cf6",
            "199088215e1e41a0b7fe574cc55ca71f",
            "e190ae7f82854a40bc45500e882a2cd6",
            "5aae2ebe108142f49b046f6e1d68475b",
            "8b8fafc9d28a430da556be6b7534bff2",
            "7f9721978e564a9fb77e2b2f2121d700",
            "b25e5bc221094de9811775bfc9d0f3d4",
            "0132c4449af6455b86b04b16d97ba551"
          ]
        },
        "id": "83uFUHzNYSlH",
        "outputId": "e1e91fd2-b6d2-4209-aebe-53b5923f1d21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Parsing nodes:   0%|          | 0/172 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53eec2c9078f443d9f0e8772728c7650"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/459 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07e8f26629014497b1237202b6d3d60e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What does this document tells about?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "addVnwXBYM8J",
        "outputId": "6a7f0f15-5fe4-42f8-83be-afe93cf23190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The document discusses various scientific studies and research findings related to climate change, biodiversity, marine ecosystems, sustainable development, and the impacts of environmental changes on different species and ecosystems. It also covers topics such as adaptation strategies, governance of high-seas resources, social and ecological risks, and the interactions between different environmental factors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding from HuggingFace Inference API"
      ],
      "metadata": {
        "id": "1541FOFZsaJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceInferenceAPIEmbedding\n",
        "# it works!!!"
      ],
      "metadata": {
        "id": "kdo9UvaLscFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "embed_model = HuggingFaceInferenceAPIEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "test_embeds = embed_model.get_text_embedding(\"Hello World!\")"
      ],
      "metadata": {
        "id": "3-wqHxBZslb-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "9ca36c6b-883b-4fb9-a4d8-eca9fb747687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "asyncio.run() cannot be called from a running event loop",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-1568e964afe9>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0membed_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHuggingFaceInferenceAPIEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"BAAI/bge-small-en-v1.5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello World!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/base/embeddings/base.py\u001b[0m in \u001b[0;36mget_text_embedding\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mCBEventType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEMBEDDING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mEventPayload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSERIALIZED\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         ) as event:\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mtext_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_text_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             event.on_end(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/embeddings/huggingface/base.py\u001b[0m in \u001b[0;36m_get_text_embedding\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mNOTE\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m \u001b[0masyncio\u001b[0m \u001b[0mevent\u001b[0m \u001b[0mloop\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcreated\u001b[0m \u001b[0minternally\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \"\"\"\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aget_text_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_text_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \"\"\"\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m     34\u001b[0m             \"asyncio.run() cannot be called from a running event loop\")\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HuggingFaceInferenceAPIEmbedding(model_name=\"BAAI/bge-small-en-v1.5\").get_text_embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "d10B4ElVdFEu",
        "outputId": "804f87f6-f57f-44ba-a7e7-3181c4368dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method BaseEmbedding.get_text_embedding of HuggingFaceInferenceAPIEmbedding(model_name='BAAI/bge-small-en-v1.5', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7f6fa0d1a440>, pooling=<Pooling.CLS: 'cls'>, query_instruction=None, text_instruction=None, token=None, timeout=None, headers=None, cookies=None, task=None)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>llama_index.core.base.embeddings.base.BaseEmbedding.get_text_embedding</b><br/>def get_text_embedding(text: str) -&gt; Embedding</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/llama_index/core/base/embeddings/base.py</a>Embed the input text.\n",
              "\n",
              "When embedding text, depending on the model, a special instruction\n",
              "can be prepended to the raw text string. For example, &quot;Represent the\n",
              "document for retrieval: &quot;. If you&#x27;re curious, other examples of\n",
              "predefined instructions can be found in embeddings/huggingface_utils.py.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 194);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(test_embeds))\n",
        "test_embeds[:10]\n",
        "# it works!!!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oh5iAc4hslU1",
        "outputId": "78c88c3f-d379-426c-a247-6657b6e2853d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "384\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.003275663824751973,\n",
              " -0.011690725572407246,\n",
              " 0.04155917093157768,\n",
              " -0.03814810514450073,\n",
              " 0.02418305166065693,\n",
              " 0.013644285500049591,\n",
              " 0.0111179044470191,\n",
              " 0.04811961576342583,\n",
              " 0.02140955626964569,\n",
              " 0.014174910262227058]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nwfKyvohslLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain Embeddings\n",
        "\n",
        "https://docs.llamaindex.ai/en/stable/examples/embeddings/Langchain.html"
      ],
      "metadata": {
        "id": "FbU7FFHfrgaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index-embeddings-langchain\n",
        "!pip install llama-index\n",
        "\n",
        "from google.colab import output\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "qZ5lqRHzrgxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "\n",
        "lc_embed_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
        ")\n",
        "embed_model = LangchainEmbedding(lc_embed_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "26b1b0495d9548fbb5c02600af0881aa",
            "01cf56354f074baf89ebbe48b1569918",
            "abea2b7c6cda45d3be65ae32f494ba54",
            "a770b3a45e0f40bf99e1173c557a003b",
            "682c77d8e4af4b3b8343e39d8b8acd5b",
            "1b6e7ff2cabe405688f20c9c76c1f875",
            "341a5c7ad75f4f0a9c10436a69c7ef3c",
            "bd74591ce5124f4785e8dbf27205a524",
            "0d96ed5907a14c15893bef81e9694c66",
            "9b70601813584e1aaf8547689f4ac67a",
            "664c7b9ac1d743528267f19687511374",
            "314fad414c8448eeb90ab89a51de43eb",
            "0251c2b2ca5b4514ab1cad77021c2fa7",
            "4057792639dc4037a2859cea7087d662",
            "7f187994f6334e0b9288cf7dcfee4042",
            "562886203ee14078bb4c98f17c59ad4a",
            "03cf676a36ca4ec5a404ab4e03ab9480",
            "bcc96a233dd04005b0bd9beeb71bbd72",
            "0e574c7aa404447c931116751e17b307",
            "cccf419cb6e341af932b3f45e19336d9",
            "f7ea2c62eb004a22853602ddc41d3865",
            "d2ecc475bd59440a88bbb8bb3d618c5a",
            "7077ececdd2f48e5a830307b7313a943",
            "e42c5d45c81d4f2b80052606e511c5c6",
            "7111e6fab0fb423dbcecce1763c4ddbc",
            "b028100a767642488ea86bc5d1ad3bd4",
            "ff72a73ab56d43f3871d0cb790cc2f5d",
            "646246c821144c0b9c6c07882e1acff2",
            "ea498c031d4844578c9bffe3318f7b55",
            "3efb3df06bfc4e42b39952c5722fc666",
            "d6dd8c964a024744be9d5ff40c46b213",
            "75c75789cba242f2b74eb784602e8718",
            "ca555e40ea314b2bad0c05dfd1bf54ec",
            "34a55a2b33ed4264909e1e727c4ece04",
            "00d468919cd34b8382660ccc2a9a9a92",
            "15307b548b074b6cb4a2a558a91f0109",
            "e476714f32324816acca467d2ee12b6a",
            "9bd8ee013b8246e2bb02f4457fb4d873",
            "fee325181ebc459692c7900d0b0c9a99",
            "2cf1ad567332408ea1b1958796c5d01d",
            "3e998518af644f958f558053d263f5b1",
            "a3eb2c0d627f46ddb93c9d91b467cf5e",
            "faaac343b37c4e36819f7fc631a23543",
            "a515bd423eff4d438b1506ca0fdd59ae",
            "407d013beb0242b0a51c3dd156c37978",
            "55663a161889442ca2a0ce15cd24516a",
            "806445bdaca54a039e06fc80e6750761",
            "cc00ae7a1b1c41778cfb2b8961eb6be9",
            "37342d0af98b4b0499bdbada13880bac",
            "a33444c3c7e3448da142933ab25051ef",
            "1b9da8fb702b4c299ba82df7d94e1334",
            "2a23933c998b414d8fb6974ccb41074a",
            "a9d9814a360c41c79f4e2238bf7b571c",
            "2dbb8e46cfec4f72955ad969a145b0e4",
            "a27af7385da748fda19e22817db9f7fb",
            "893ddb1b2eec4f83adcd27626701aff0",
            "3f9be6e16fd74d60a90cfc548577a94d",
            "d168d8558f1244089ba0890b2b8ffd83",
            "693176cf292642dfaafd6be5227f8b19",
            "b2f5c1cf4b1d4ca2a3870985268b6350",
            "621da2628446486db130d6b6a9c94ee4",
            "d07b1fb9b74842ec94686dc62664c84d",
            "fa077e02d02443ce804e02740550abd5",
            "b25539b7f3df466d92a78bfd8a7b2fb5",
            "e9c1b9dc17724fa79eac00d4e93dc526",
            "1d84346a8033488681f6572f5b7e7439",
            "625811c4d16e41f2913ce35894c4de11",
            "8c330406f54f4c3fa599f1a8daf43e8c",
            "7b5ebab2faa548b19ebb8164a44b469c",
            "943b90235fe143aa89d87edfd624915a",
            "c3d0db6702c44709ba47d78286eeaf66",
            "ff22497d72e845d2b2bdf82553ad7f30",
            "8a69b6c634aa43ad8f65d55b18d8fc55",
            "6333dfbe956b4bcfb66fad65a190c27d",
            "22b8c31c0a274925af82fe7b6acd4ab6",
            "1a25d4caad6145ff9e00e1afe7a4b7a1",
            "fd7b154d93c1455182dd2c6bdce91f6f",
            "269ffa6c44ec4900bf4cd48787993e1a",
            "51236de60fc4499e9c2dbf74ec16e8d1",
            "c6c627efcc724eb1bf7818736255331c",
            "a1134ed25ac34476850e901848be9b28",
            "d0a675b299974cceba0f57d133cc7938",
            "35768a315830455ea544161e0099cb2e",
            "d2bd392f60e54128ad454469b0890c6c",
            "01878687ff0c446caeb7a1e430cc6ea5",
            "0c7ff2bb33d4416aa7cb41bc94df93e6",
            "58a61e0090b340648bba2a5db37a9321",
            "52265d48446b462d81035ff114e326e4",
            "ee455263d427496b8e0e8930885a0708",
            "6b0ed029d2244739b95a6de5b1cc4af7",
            "4b9b2881a27b45b8a4b1541c0b6e367b",
            "227fcb726aa94530bb5e5fe852d47928",
            "d5ac0e4c34554af585ed04bacb5faf20",
            "7b533d1764574536909e2d57e20d4487",
            "2a107bb04cfd4f6a914e7e201d1c8d09",
            "b6249f6e59d243a7b1ec96544499f613",
            "49a01994eedd46e5befeeb49a46b808f",
            "5d64e1873d6a43a18b5186cf07ca3657",
            "0b040691d0704805b410026b2355ac77",
            "85480f6a48c144dfb4fe6846d3356a0b",
            "71a647aa31144914aaf609fecd5955bc",
            "c830108f49de432ba959a66fb6c0beaf",
            "b57a5a4859454bafb4de0184cea1e151",
            "426c72c7836d46b3a952b1218dff800a",
            "e4cb64f745cb471d829043ff03eb24e7",
            "642fefb327ee40c99b7026a0c004d488",
            "ba185eeeb70f4677a4017e15c631843e",
            "19528a0b68594e70b2d7f7127f63d0ff",
            "3fa58ddefdde493a8f9e47b447b3b786",
            "11a802cc75ec42fd80d6c92fb862f88a",
            "b0dc336f13a14e529fea553e8f42d844",
            "f1f0850675ea4699b21f02b645214a56",
            "004e40597f5d490f92b053b0bcc97212",
            "e433fa87a39c4987b0d79144b1f4357a",
            "fea4afcc04aa40138b48c10da94682ec",
            "4eba7acde90549a899f31b616679fd97",
            "c5b5cc36c86342d08d3ceb1355387042",
            "7912953cf38a47a3ba7f6cc4ca752d87",
            "b5595089045141ada9a4128159074d5e",
            "ade9f6b49e15411c9c4b3aaf3dc6786b",
            "d377cf7e2ef6483f8a8432618fc77156"
          ]
        },
        "id": "TnJlSYB2rkXA",
        "outputId": "55bddc61-1841-46e9-905e-a6c88c4a028c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26b1b0495d9548fbb5c02600af0881aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "314fad414c8448eeb90ab89a51de43eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7077ececdd2f48e5a830307b7313a943"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34a55a2b33ed4264909e1e727c4ece04"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "407d013beb0242b0a51c3dd156c37978"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "893ddb1b2eec4f83adcd27626701aff0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "625811c4d16e41f2913ce35894c4de11"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "269ffa6c44ec4900bf4cd48787993e1a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee455263d427496b8e0e8930885a0708"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85480f6a48c144dfb4fe6846d3356a0b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0dc336f13a14e529fea553e8f42d844"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic embedding example\n",
        "embeddings = embed_model.get_text_embedding(\n",
        "    \"It is raining cats and dogs here!\"\n",
        ")\n",
        "print(len(embeddings), embeddings[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hup1xsBCrlaH",
        "outputId": "b02d3408-2e6e-4036-ba3c-bc556f8347ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "768 [-0.005906173028051853, 0.04911916330456734, -0.04757879301905632, -0.04320327565073967, 0.02837086096405983, -0.01737167499959469, -0.04422018676996231, -0.01903551258146763, 0.049416132271289825, -0.038391221314668655]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI Embeddings\n",
        "\n",
        "https://docs.llamaindex.ai/en/stable/examples/embeddings/OpenAI.html"
      ],
      "metadata": {
        "id": "U8uJ_-0S1dX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index-embeddings-openai\n",
        "!pip install llama-index\n",
        "\n",
        "from google.colab import output\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "zW0d_R_NrmuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "\n",
        "embed_model = OpenAIEmbedding(embed_batch_size=10)\n",
        "Settings.embed_model = embed_model"
      ],
      "metadata": {
        "id": "pCByH9LV1ja9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get API key and create embeddings\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "# embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\") # give 3072 dimensions\n",
        "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\") # gives 1536 dimensions\n",
        "# embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\", dimensions=512) # give 512 dimensions\n",
        "\n",
        "embeddings = embed_model.get_text_embedding(\n",
        "    \"Open AI new Embeddings models is great.\"\n",
        ")"
      ],
      "metadata": {
        "id": "VS11PS0w1jWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(embeddings))"
      ],
      "metadata": {
        "id": "zvr3nl0Q1wOm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94fb8ebd-323a-4812-c10c-3d15928ab713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(embeddings[:10])"
      ],
      "metadata": {
        "id": "1bdQEaFTTcHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantized Model - LlamaCPP\n",
        "\n",
        "Models:\n",
        "- TheBloke/Mistral-7B-Instruct-v0.2-GGUF\n",
        "\n",
        "References:\n",
        "- https://docs.llamaindex.ai/en/stable/examples/llm/llama_2_llama_cpp.html"
      ],
      "metadata": {
        "id": "oiB5d_M64fNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index-embeddings-huggingface\n",
        "%pip install llama-index-llms-llama-cpp\n",
        "!pip install llama-index\n",
        "\n",
        "from google.colab import output\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "40Z8p2A64lJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.llms.llama_cpp import LlamaCPP\n",
        "from llama_index.llms.llama_cpp.llama_utils import (\n",
        "    messages_to_prompt,\n",
        "    completion_to_prompt,\n",
        ")"
      ],
      "metadata": {
        "id": "OrZFRu9oPs4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gpTNelVGMJH",
        "outputId": "bd54fa85-c84f-44be-b5b5-a513729927d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf to /root/.cache/huggingface/hub/tmpf_4quad_\n",
            "mistral-7b-instruct-v0.2.Q4_K_M.gguf: 100% 4.37G/4.37G [00:33<00:00, 132MB/s]\n",
            "./mistral-7b-instruct-v0.2.Q4_K_M.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LlamaCPP(\n",
        "    # You can pass in the URL to a GGML model to download it automatically\n",
        "    # model_url=\"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q4_0.bin\",\n",
        "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
        "    model_path=\"/content/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=256,\n",
        "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
        "    context_window=3900,\n",
        "    # kwargs to pass to __call__()\n",
        "    generate_kwargs={},\n",
        "    # kwargs to pass to __init__()\n",
        "    # set to at least 1 to use GPU\n",
        "    model_kwargs={\"n_gpu_layers\": 1},\n",
        "    # transform inputs into Llama2 format\n",
        "    messages_to_prompt=messages_to_prompt,\n",
        "    completion_to_prompt=completion_to_prompt,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RYlArG-P14-",
        "outputId": "d7af56c3-3a30-4995-9892-c4118b5c3de1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /content/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 1000000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "llm_load_tensors: offloading 1 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 1/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =   132.50 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 3900\n",
            "llama_new_context_with_model: freq_base  = 1000000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:  CUDA_Host KV buffer size =   472.27 MiB\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =    15.23 MiB\n",
            "llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB\n",
            "llama_new_context_with_model:  CUDA_Host input buffer size   =    16.65 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =   283.75 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 3\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Guessed chat format: mistral-instruct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.complete(\"Hello! Can you tell me a poem about cats and dogs?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KBfYKEPP10U",
        "outputId": "4acef511-53da-487a-fdd4-4152776187c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =    1704.74 ms\n",
            "llama_print_timings:      sample time =     164.91 ms /   256 runs   (    0.64 ms per token,  1552.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1704.45 ms /    76 tokens (   22.43 ms per token,    44.59 tokens per second)\n",
            "llama_print_timings:        eval time =  177787.95 ms /   255 runs   (  697.21 ms per token,     1.43 tokens per second)\n",
            "llama_print_timings:       total time =  180748.96 ms /   331 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Certainly! Here's a light-hearted poem about the friendship between cats and dogs:\n",
            "\n",
            "In a world where fur meets fur,\n",
            "Where playful paws and purrs do stir,\n",
            "There's an unlikely bond that's pure,\n",
            "Between the cat and the dog, it's true allure.\n",
            "\n",
            "The cat with grace and elegance,\n",
            "Sleek and slender, with a gentle sense,\n",
            "And the dog with heart so vast and wide,\n",
            "In their differences, they find their stride.\n",
            "\n",
            "The cat with eyes that gleam and glint,\n",
            "In the sunbeam's warm and gentle hint,\n",
            "And the dog with wagging tail so bright,\n",
            "Basking in the joy of day and night.\n",
            "\n",
            "They frolic in the fields of green,\n",
            "Chasing butterflies in a serene scene,\n",
            "And when the day is through and night descends,\n",
            "They curl up close, their bond never ends.\n",
            "\n",
            "So here's to cats and dogs, so different yet the same,\n",
            "In their unique and beautiful, wondrous game,\n",
            "May their friendship be a source of endless delight,\n",
            "A testament to love\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_iter = llm.stream_complete(\"Can you write me a poem about fast cars?\")\n",
        "for response in response_iter:\n",
        "    print(response.delta, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8BWAD-pP6Qj",
        "outputId": "4063eae4-6670-44c3-b294-e1dbca70957a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " In the realm where the asphalt meets the sky,\n",
            "Where horsepower reigns and time seems to fly,\n",
            "Lie the kings of the road, the fast cars, so sly,\n",
            "Their engines roaring, their tires never shy.\n",
            "\n",
            "Through the curves they dance with grace and might,\n",
            "Their frames sculpted by the hands of skilled artisans,\n",
            "A symphony of speed in the cool of the night,\n",
            "Their headlights cutting through the darkness like swans.\n",
            "\n",
            "With every rev of their powerful hearts,\n",
            "They leave the world behind in their wake,\n",
            "A blur of color, a work of art,\n",
            "Their beauty and power, an intoxicating cake.\n",
            "\n",
            "So if you're ever feeling small and lost,\n",
            "Just close your eyes and let the roar of the engine be your guide,\n",
            "Let the wind whip through your hair, your heart unthawed,\n",
            "And let the fast cars take you on a wild, exhilarating ride."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =    1704.74 ms\n",
            "llama_print_timings:      sample time =     133.60 ms /   216 runs   (    0.62 ms per token,  1616.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8131.57 ms /    14 tokens (  580.83 ms per token,     1.72 tokens per second)\n",
            "llama_print_timings:        eval time =  147136.95 ms /   215 runs   (  684.36 ms per token,     1.46 tokens per second)\n",
            "llama_print_timings:       total time =  156752.99 ms /   229 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG Implementation: Chatting to Files Using Quantized Mistral-7B\n",
        "\n",
        "https://www.youtube.com/watch?v=1mH1BvBJCl0&list=WL&index=216"
      ],
      "metadata": {
        "id": "zIwKF6L0D47O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axpOvjufh1zm"
      },
      "outputs": [],
      "source": [
        "!pip install -q pypdf python-dotenv transformers\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir\n",
        "%pip install llama-index-llms-huggingface\n",
        "!pip install -q llama-index\n",
        "!pip install -q sentence-transformers\n",
        "!pip install langchain langchain-community\n",
        "\n",
        "from google.colab import output\n",
        "output.clear()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "snx-DC5kXzak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import logging\n",
        "# import sys\n",
        "\n",
        "# logging.BasicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
      ],
      "metadata": {
        "id": "cxRrfSwPVI4_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "666e6af3-974b-4770-d9d4-a4a17eda114a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'logging' has no attribute 'BasicConfig'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-3aa849069d71>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'logging' has no attribute 'BasicConfig'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "# Fix: https://stackoverflow.com/questions/77984729/importerror-cannot-import-name-vectorstoreindex-from-llama-index-unknown-l"
      ],
      "metadata": {
        "id": "qXkbj3jCVl4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(\"/content/Data\").load_data()"
      ],
      "metadata": {
        "id": "twZ7RUgsVIyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)\n",
        "documents[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItMOdb9yd96q",
        "outputId": "920f90e0-f3e2-4bf1-b481-c440ff5d4c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(id_='fb5e7b71-89bc-4651-b2ec-f8cf05bcb84a', embedding=None, metadata={'file_path': '/content/Data/ds.txt', 'file_name': '/content/Data/ds.txt', 'file_type': 'text/plain', 'file_size': 1542, 'creation_date': '2024-03-03', 'last_modified_date': '2024-03-03'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Perkembangan pesat di bidang teknologi diikuti pula oleh perkembangan jumlah data. Hal ini terjadi di berbagai bidang ilmu. Oleh karena itu dibutuhkan suatu disiplin ilmu yang dapat digunakan untuk memproses jumlah data yang semakin masif. Disiplin ilmu tersebut adalah data science (Jifo & Lingling, 2014). Data science merupakan cabang ilmu yang mempelajari berbagai metode dan teknik yang dapat digunakan untuk menarik manfaat dari data. Beberapa proses untuk mengolah data yang dicakup antara lain adalah teknik mengumpulkan dan membersihkan data, melakukan praproses pada data, menganalisis data, dan menginterpretasikan data.\\r\\n\\r\\n \\r\\nGambar 2.1. Data Science sebagai penghubung antar bidang matematika & statistika, komputasi & IT, serta domain knowledge\\r\\n(Zadeh, A. Y. & Shahbazy, M., 2020)\\r\\n\\r\\n\\tProses mengambil manfaat dari data itu sendiri membutuhkan pengetahuan dari beberapa cabang ilmu yang berbeda. Namun secara garis besar, ada tiga bidang ilmu yang membentuk istilah data science. Ketiga bidang ilmu tersebut adalah matematika dan statistika, ilmu komputer, dan domain expert atau ahli khusus. Ilmu matematika dan statistika digunakan untuk menjadi landasan teori dalam melakukan pengolahan, pemrosesan, dan analisis data. Ilmu komputer digunakan untuk mengimplementasikan teori-teori yang diperlukan untuk mengolah data. Sedangkan ilmu domain expert dibutuhkan untuk menginterpretasi dan memvalidasi hasil analisis data yang dilakukan. Gambar 2.1 mengilustrasikan hubungan interseksi antara ketiga bidang ilmu yang disebutkan\\r\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.llms import LLM\n",
        "from llama_index.core.llms.chatml_utils import messages_to_prompt, completion_to_prompt\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM"
      ],
      "metadata": {
        "id": "MODjbZbVViM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# from llama_index.llms import LlamaCPP\n",
        "# from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
        "\n",
        "from llama_index.llms.llama_cpp import LlamaCPP\n",
        "from llama_index.llms.llama_cpp.llama_utils import messages_to_prompt, completion_to_prompt\n",
        "\n",
        "llm = LlamaCPP(\n",
        "    # You can pass in the URL to a GGML model to download it automatically\n",
        "    model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf',\n",
        "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
        "    model_path=None,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=256,\n",
        "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
        "    context_window=3900,\n",
        "    # kwargs to pass to __call__()\n",
        "    generate_kwargs={},\n",
        "    # kwargs to pass to __init__()\n",
        "    # set to at least 1 to use GPU\n",
        "    model_kwargs={\"n_gpu_layers\": -1},\n",
        "    # transform inputs into Llama2 format\n",
        "    messages_to_prompt=messages_to_prompt,\n",
        "    completion_to_prompt=completion_to_prompt,\n",
        "    verbose=True,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbtjjBfrBm2I",
        "outputId": "e2d122c1-d221-4e7a-ccd0-d20aef2023ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading url https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf to path /tmp/llama_index/models/mistral-7b-instruct-v0.1.Q4_K_M.gguf\n",
            "total size (MB): 4368.44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4167it [00:36, 113.35it/s]                         \n",
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /tmp/llama_index/models/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  4095.05 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 3900\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB\n",
            "llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB\n",
            "llama_new_context_with_model:  CUDA_Host input buffer size   =    16.65 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using fallback chat format: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "\n",
        "embed_model = LangchainEmbedding(\n",
        "  HuggingFaceEmbeddings(model_name=\"thenlper/gte-large\")\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461,
          "referenced_widgets": [
            "b5803b21448c4bd0b91f7401d533b4e6",
            "9f4b919d6710422ba79a22fb3a12aa88",
            "8c6dcc139a03465cb90df1658122574a",
            "3f66a56f56c046338a59f617c9d82fba",
            "2fa05761460b4c72ac471c59e2eb458d",
            "c72553b6c51042a5a7ca47093a84cc80",
            "781c3ab9a1ec492fbc67421f5ad471ae",
            "878a83f76cc74c78a70dc680adda5d37",
            "0860eb8724c44afbbdf24a525268caf7",
            "10d5eddddb4c40bba9b46cb208744535",
            "ffcf65681c0f49aa9b9d1b31a8418674",
            "24fbd60e08c74936ae1be4c8f06bef65",
            "38c997d79e9448b08644d7bcab631e78",
            "1dcea7727c2d46cba466dc88af81adbc",
            "5a7fda1de56147faa51f82cb84ff50c0",
            "7d3e9752b6fb47cbbde3925e9fbbbff9",
            "9dbbc5b182c84afa91767b14167e2e9c",
            "57a698015d904883b2149da2a3ef50e1",
            "2eee96ddbc1943c38570f7f533c4129e",
            "8ee3b6e323704690a9ea04f83a123b8d",
            "36949da81d7e4ffb9b67a0de3ace0837",
            "fa54a29256dc46138056635267334aea",
            "4135c5b2b872490ab5e6974261ef8655",
            "a24cb92fe7cd4f17aaa62f5d845dfb46",
            "d1043cf9cc28479287764c2a16271304",
            "565dc518cce641a0b86caf2140202e88",
            "25f94f913dd74969bcfadf039aaaf14c",
            "53190b294401495f8d2a05456191b637",
            "31b7f2e719974a849e4f19646b454ea6",
            "244734811b4f4934a168b9d96a8a1a73",
            "a66234800cde466d91d718905efdab3d",
            "4a12e7b2d53846f5b44d55317444deb9",
            "65907486877d42b39b2d8912610fbefb",
            "47092cf8870d4451931e9df8eef910dc",
            "3589e0af216d4dda931b5558200bb609",
            "de797f7e489c4db0bca6e5c605ff08da",
            "a99b39c2ff4547189b0f88ba5eec199a",
            "05eaa138c08442e88e17933704de7660",
            "85cbc8e2026f4fb1878f08f8eeb74ead",
            "30e2f9614bdb48b5a8c074bb5a2c345c",
            "f4899f0990e24654a08fabc52baa39a3",
            "d1329b22904c407cb4746576ea5ebfde",
            "43841d9ea5a547128fca6932894e12d4",
            "e974d08463d841a188fd924b2d4a95fe",
            "5494942503764c01bbd2a61b5745931b",
            "3ab9f4c8a0304607bd443ec03ef86738",
            "db8fe01da0144963a83027901684d802",
            "03a32ffe254a4ace969db4f5dfa2efcc",
            "91efa7d6d9194da0af8f2ea4f469da8a",
            "3e9f3b532e694822bd61c6ef759d04f9",
            "5d9559d0a7ce426c9f3b6cdc5ebeb903",
            "e578fd35c4a4469c91aeffd9212f122d",
            "5d718e5f40b6405c9deafcee2168cb10",
            "ceea83de5080469783800ef845ff7330",
            "58823d198f2848bda4cd9dc2861f702c",
            "00fd1b1448094ef3bb0498b511724e08",
            "07f3e13853c0455fbab326b22c4128b7",
            "ef09d9cc7be546058c8a87f4d6f32960",
            "78d5f57310fd43dcb529d3d217b76d0b",
            "b0fcaa0481a64d58a325537d9d74cfb7",
            "db7bd6d15c42453a875ecee853dd5448",
            "a973b9a0d57a4ced894a9ff970ff904b",
            "c82b6a84446546d98f34da464691b0f8",
            "74d6e38ca04f45cf9e09fe04f29af34c",
            "20a529418f7a4648be4f77af49458bb2",
            "c7e976ff930d44a78df7fe85d9459ca0",
            "5103bb7e84564202a39c93b6f63502ee",
            "a25425d9d0a54ee28619489a2bfab1d7",
            "439844ab21254eac90d77854bde1d69c",
            "f0e4df903b3f468197c1b57e7d47b605",
            "b0cb1318471c461e9573b8bcdfd72920",
            "6699e8e3e92a42429f90565397e9459d",
            "8d9ca49c5e7c4a25a596031a774444da",
            "08b3499adbba4143a22de7b53362cf18",
            "919ec85e5f7e4b748728aad6fb5a0612",
            "70b86756bd194c5bb94d5c8cb4fe5f8f",
            "ad3558af7d74428baf9b7b42644d8123",
            "684530a62c7d4c788666175f8024ea7d",
            "554631f6338b4bcb850bdbc7e37616dc",
            "51f8a3a6a7f246c8af4b6f5c23995dbf",
            "1c0d4d11a2c844d7ac7e5eba0faa5bfa",
            "48b40cdfd1974a0781b20982b34415dc",
            "cd63a386def540b798e3202015464759",
            "cd05596dbee64d519c2584fba4877883",
            "f44a8c236c964ea7831cbb06dbb1a7e1",
            "2b29244fc19540d2b34181faf854d791",
            "ad72f749c29b42d198a9dcdb53d78974",
            "d6d8b8b8a91b4dd991478934873da7ff",
            "e28454b80566413097d418286150b3ea",
            "bbba4b0500fb4d48879453f0e218513c",
            "9768a3243bb84307afb875d597dce24c",
            "eff4a7b80cc5467da08ecaa56d85203e",
            "1f2675c0c99940b9b632e84f94f95d16",
            "49deaf7c714b4147a69446f1e08ce8ef",
            "9b974a7ca67c427da3de540e99015257",
            "d9cddb9b786448dbb922f4e72138098c",
            "a13e24b9b5134769bb6797a96f30b2ae",
            "f6aee8ad6a34438ba69b6c58d1106655",
            "9e4a6c844b9c4bfbb53429294b5737a2",
            "fad9ca3d0f3f405c85585938def28840",
            "3aff280f7af3470ab16a9953cf8e39cb",
            "042eb45d93c3457094b40a2794614358",
            "e819c7a0dce2456f83670a21fd3c12ad",
            "753ddef995d4477799ca83eb7dac10e3",
            "763ce10978ca4fbd8121b03456d285c6",
            "d21e3d206e944c9aa01b98befae18ac1",
            "af735283816a4c3985e46532c47ac64e",
            "67851a74eab84f5d943ad0677b8badee",
            "4dff626ad98e4b769d3d6b29ef50971d",
            "be8cdc972da64d898a5ebf5314071922"
          ]
        },
        "id": "7jqdx5TDCLnI",
        "outputId": "36f2c1f7-3194-4f48-96de-1cfc65d95a88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5803b21448c4bd0b91f7401d533b4e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/67.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24fbd60e08c74936ae1be4c8f06bef65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4135c5b2b872490ab5e6974261ef8655"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47092cf8870d4451931e9df8eef910dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/670M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5494942503764c01bbd2a61b5745931b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00fd1b1448094ef3bb0498b511724e08"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5103bb7e84564202a39c93b6f63502ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "684530a62c7d4c788666175f8024ea7d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e28454b80566413097d418286150b3ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fad9ca3d0f3f405c85585938def28840"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "service_context = ServiceContext.from_defaults(\n",
        "    chunk_size=256,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE9ryw9pCMe-",
        "outputId": "22286e35-12e4-406e-a935-7c7102f9140f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-7e47785a9da0>:1: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
      ],
      "metadata": {
        "id": "5DaQy9h0DMXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What is Machine Learning?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KETKrfvSDMTG",
        "outputId": "3c89ee1e-9b89-44fb-a24f-44bc33fd63a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     668.32 ms\n",
            "llama_print_timings:      sample time =     143.88 ms /   205 runs   (    0.70 ms per token,  1424.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_print_timings:        eval time =    5710.67 ms /   205 runs   (   27.86 ms per token,    35.90 tokens per second)\n",
            "llama_print_timings:       total time =    6880.19 ms /   206 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Machine learning (ML) is a subfield of Artificial Intelligence (AI) that involves using algorithms to systematically combine relationships between data and information (Awad & Khanna, 2015). It is one of the methods that can be used to process data. In machine learning, there are three types or methods of learning, which are supervised learning, unsupervised learning, and reinforcement learning (Colliot, 2023). Supervised learning involves focusing on mapping data input X to data label as output y. An algorithm or machine learning model will learn the patterns in the training data that consists of pairs of input X and output y. Unsupervised learning involves training a model to learn patterns in the data without any label to serve as a reference for the model to learn from (Colliot, 2023). Reinforcement learning focuses on maximizing the reward obtained based on the conditions of the learning environment (Colliot, 2023).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"Apa yang menjadi subbidang dari kecerdasan buatan?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s93ohdWWDJJr",
        "outputId": "d2d7e88a-4c61-4a08-92a1-0b03312e9116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     668.32 ms\n",
            "llama_print_timings:      sample time =      43.45 ms /    77 runs   (    0.56 ms per token,  1772.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =     877.82 ms /   626 tokens (    1.40 ms per token,   713.13 tokens per second)\n",
            "llama_print_timings:        eval time =    2121.67 ms /    76 runs   (   27.92 ms per token,    35.82 tokens per second)\n",
            "llama_print_timings:       total time =    3288.45 ms /   702 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Based on the provided context information, subbidang dari kecerdasan buatan adalah matematika dan statistika, ilmu komputer, dan domain expert atau ahli khusus. These three disciplines form the basis of data science, which involves various techniques for processing, analyzing, and interpreting data to extract useful insights.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"Jawab menggunakan teks yang diberikan. Apa perbedaan ensemble learning dan machine learning?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJfQQ1rkENJ_",
        "outputId": "88f3c6ff-8891-4fd6-ce1b-20565b9e2298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     668.32 ms\n",
            "llama_print_timings:      sample time =      79.60 ms /   114 runs   (    0.70 ms per token,  1432.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =     273.27 ms /    36 tokens (    7.59 ms per token,   131.74 tokens per second)\n",
            "llama_print_timings:        eval time =    3260.51 ms /   113 runs   (   28.85 ms per token,    34.66 tokens per second)\n",
            "llama_print_timings:       total time =    4151.36 ms /   149 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Ensemble learning is a subfield of machine learning that involves combining multiple models to create a more accurate and robust model. Machine learning, on the other hand, refers to the process of training models to make predictions or decisions based on data. Ensemble learning can be used for both training and inference processes, while machine learning typically only involves training processes. In ensemble learning, weak learners (models that are simple and easy to train) are combined to create a stronger model, while in machine learning, models are trained independently and then combined to create an ensemble.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Referensi\n",
        "\n",
        "- https://twitter.com/llama_index/status/1762158562657374227\n",
        "- Talk to Your Documents, Powered by Llama-Index\n",
        "  - https://www.youtube.com/watch?v=WL7V9JUy2sE&list=WL&index=214&pp=gAQBiAQB\n",
        "- RAG Implementation Medical Chatbot with Mistral 7B LLM LlamaIndex GTE Colab Demo\n",
        "  - https://www.youtube.com/watch?v=1mH1BvBJCl0&list=WL&index=215&pp=gAQBiAQB\n",
        "- Building A RAG System with Gemma, MongoDB and Open Source Models\n",
        "  - https://huggingface.co/learn/cookbook/rag_with_hugging_face_gemma_mongodb\n",
        "- Effortless Company Research Using Open Source — Using Llama Index, Huggingface Embeddings & Llama 2 LLM On News\n",
        "  - https://medium.com/scrapehero/effortless-company-research-using-open-source-using-llama-index-huggingface-embeddings-llama-1725a60da117\n",
        "- https://docs.llamaindex.ai/en/stable/examples/llm/mistralai.html\n",
        "- https://docs.llamaindex.ai/en/stable/examples/customization/llms/SimpleIndexDemo-Huggingface_camel.html\n",
        "- Models\n",
        "  - https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF\n",
        "  - https://huggingface.co/GritLM/GritLM-7B"
      ],
      "metadata": {
        "id": "zvRA4FDudkq_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TgIVM62r4JRG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}